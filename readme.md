# ET-data Secondary Face Classifier – IP PAN

This project provides a command-line interface for running a **secondary face classification model** on eye-tracking recordings **acquired using Pupil Labs hardware**.  
It is intended **only as a classifier** and relies on a **pretrained CNN model** (no training included).

---

## Pipeline Overview

For each recording, the pipeline performs the following steps:

1. **Frame extraction**  
   Crops candidate face regions from the scene video using precomputed face detections generated by the *Pupil Labs Face Mapper*.

2. **Classification**  
   Applies a pretrained CNN to perform secondary face / non-face classification on the extracted frames.

3. **Postprocessing**
   - Temporal smoothing of per-frame predictions  
   - Generation of `augmented_face_frames.csv` with refined classifications

4. **Optional HTML visualization**  
   Generates simple, paginated HTML pages for quick visual inspection of face and non-face frames.

After all recordings are processed, the pipeline can **aggregate results across recordings** and **prune gaze/fixation CSVs** accordingly.

---
# Example Setup (Ubuntu & Windows)

## 1. Clone the project repository

**Ubuntu / Linux / WSL:**

```bash
# Install Git LFS if not installed
sudo apt update
sudo apt install git-lfs
git lfs install

# Clone the repository
git clone https://github.com/m-wawrzyniak/PAN_secondary_classifier_interface
cd PAN_secondary_classifier_interface

# Pull actual model files via Git LFS
git lfs pull
```

**Windows:**

1. Install **Git**: [https://git-scm.com/download/win](https://git-scm.com/download/win)  
2. Install **Git LFS**: [https://git-lfs.github.com/](https://git-lfs.github.com/)  
3. Open **PowerShell** and run:

```powershell
git lfs install
git clone https://github.com/m-wawrzyniak/PAN_secondary_classifier_interface
cd PAN_secondary_classifier_interface
git lfs pull
```

>  **Important:** If you downloaded the ZIP from GitHub instead of cloning, the model file will be missing. You **must use Git + Git LFS** to obtain the 300 MB+ checkpoint.

---

## 2. Ensure Python ≥ 3.10 is installed

**Check Python version:**

```bash
python3 --version  # Ubuntu / Linux
python --version   # Windows
```

If Python is not installed, download it:

- [Python for Windows](https://www.python.org/downloads/release/python-31014/)  
- Ubuntu / Linux:  

```bash
sudo apt update
sudo apt install python3 python3-venv python3-pip
```

---

## 3. Create a virtual environment

**Ubuntu / Linux / WSL:**

```bash
python3 -m venv .sfc_venv
```

**Windows:**

```powershell
python3 -m venv .sfc_venv
```

---

## 4. Activate the virtual environment

**Ubuntu / Linux / WSL:**

```bash
source .sfc_venv/bin/activate
```

**Windows (CMD):**

```cmd
.sfc_venv\Scripts\activate.bat
```

**Windows (PowerShell):**

```powershell
.sfc_venv\Scripts\Activate.ps1
```

---

## 5. Install dependencies

```bash
pip install -r requirements.txt
```

Check that the packages installed correctly:

```bash
pip list
```

---

## 6. Download input data

You need **two sources of input**:

1. **Scene videos and timeseries CSVs**  
   - Path: `Workspace → Project → Downloads → Timeseries CSV and Scene Video`  
   - Each recording should be in a separate folder

2. **Face Mapper enrichment output**  
   - Path: `Workspace → Project → Downloads → <Enrichment Name>`  
   - Must contain:
     - `sections.csv`
     - `face_detections.csv`

---

## 7. Run the pipeline

From the **project root directory**:

**Ubuntu / Linux / WSL:**

```bash
python3 -u source/sec_classification.py \
    --rec_dir_root "/path/to/recordings" \
    --output_root "/path/to/output" \
    --data_root "/path/to/data" \
    --run_range 1 3 \
    --aggregate \
    --html \
    > run.log 2>&1
```

**Windows (CMD / PowerShell) — one line, no line continuation:**

```powershell
python -u source\sec_classification.py --rec_dir_root "\path\to\recordings" --output_root "\path\to\output" --data_root "\path\to\data" --run_range 1 3 --aggregate --html > run.log 2>&1
```

### Command-line arguments

| Argument | Description |
|---------|-------------|
| `--rec_dir_root` | Folder containing **Timeseries CSV and Scene Video** data. Each recording must be in a separate subfolder. |
| `--output_root` | Output directory for extracted frames, intermediate data, and results. **Note:** intermediate data may exceed 10 GB. |
| `--data_root` | Folder containing **Face Mapper enrichment** files (`sections.csv`, `face_detections.csv`). |
| `--run_range` | Two integers `[start end]` selecting pipeline stages: 1=extract, 2=classify, 3=postprocess. Default: `[1 3]`. |
| `--aggregate` | Aggregate all per-recording results into a single CSV after processing (recommended). |
| `--smooth_window` | Temporal smoothing window (odd integer, e.g. `3`). Use `0` to disable smoothing. Default: `3`. |
| `--html` | Generate HTML visualizations for manual inspection (recommended). |

---

## 8. Monitor progress

**Ubuntu / Linux / WSL:**

```bash
tail -f run.log
```

**Windows (PowerShell):**

```powershell
Get-Content .\run.log -Wait
```

**Windows (CMD alternative):**

```cmd
type run.log
```

---


## Output Structure

For **each recording**, a folder will be created inside `output_root` containing:

- Hundreds of extracted face-frame images
- `model_class.csv` – raw per-frame model predictions (`is_face = 0/1`)
- `face_frames.csv` – intermediate data used internally by the pipeline
- `augmented_face_frames.csv` – final, postprocessed classification results for this recording
- `result_html/` – HTML pages for visual inspection (only if `--html` was used)

If the pipeline is run with `--aggregate`, additional files are created in the **project directory**:

- `aggregated/augmented_face_detections.csv` – combined results from all recordings
- If smoothing is enabled (`--smooth_window != 0`):
  - `fixations_on_faces.csv`
  - `gaze_on_face.csv`

---

## License

This project is licensed under the MIT License. See the LICENSE file for details.

